{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsoni03/AI-projects/blob/main/Personal_Deep_Convolutional_Q_Learning_for_Pac_Man.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAiHVEoWHy_D"
      },
      "source": [
        "# Deep Convolutional Q-Learning for Pac-Man"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjO1aK3Ddjs5"
      },
      "source": [
        "## Part 0 - Installing the required packages and importing the libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwdRB-ZLdrAV"
      },
      "source": [
        "### Installing Gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbnq3XpoKa_7",
        "outputId": "40a7e527-6a3d-4b59-a7f7-e63b15889029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "\u001b[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: ale-py>=0.9 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.10.1)\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.10/dist-packages (from ale-py) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py) (4.12.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1.post0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!pip install ale-py\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39ks55NbGdVI"
      },
      "source": [
        "This deep convolutional q learning model does not solve every environment and this means that it will needs an environment that is partial deterministic makes it less complex and easier to compute the convolution model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-wes4LZdxdd"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho_25-9_9qnu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7wa0ft8e3M_"
      },
      "source": [
        "## Part 1 - Building the AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlYVpVdHe-i6"
      },
      "source": [
        "### Creating the architecture of the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIOjKqMSJObw"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self, action_size, seed = 42):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    # set the seed\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    # create convulation channel which takes in 3rgb (input), output channels for pacman is 32, kernel size, stride\n",
        "    # layer 1\n",
        "    self.conv1 = nn.Conv2d(3, 32,  kernel_size = 8, stride = 4)\n",
        "    # batch normalization, number of features from previous layers\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "    # second convulation chanel taking previous and increaing output, kernal size and stride decreases\n",
        "    self.conv2 = nn.Conv2d(32, 64,  kernel_size = 4, stride = 2)\n",
        "    # batch normalization, number of features from previous layers\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "    # third and will be 64 for the convulation\n",
        "    self.conv3 = nn.Conv2d(64, 64,  kernel_size = 3, stride = 1)\n",
        "    # batch normalization, number of features from previous convulation(output)\n",
        "    # outputs 64 feature maps\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "    # fourth convulation and increasing it to 128 for output layers\n",
        "    self.conv4 = nn.Conv2d(64, 128,  kernel_size = 3, stride = 1)\n",
        "    # batch normalization, number of features from previous convulation(output)\n",
        "    self.bn4 = nn.BatchNorm2d(128)\n",
        "    # final output of the flattening layer\n",
        "    # output size from all convlations is input, output will 512\n",
        "    # Converting the multi-dimensional tensor into a 1-dimensional tensor to expecting a 1d\n",
        "    self.fc1 = nn.Linear(10* 10 * 128, 512)\n",
        "    # height * weight * channels (output) and vectore of size of 512\n",
        "    # input will be previous\n",
        "    self.fc2 = nn.Linear(512, 256)\n",
        "    self.fc3 = nn.Linear(256, action_size)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = F.relu(self.bn1(self.conv1(state)))\n",
        "    x = F.relu(self.bn2(self.conv2(x)))\n",
        "    x = F.relu(self.bn3(self.conv3(x)))\n",
        "    x = F.relu(self.bn4(self.conv4(x)))\n",
        "    # flattens the tensor\n",
        "    # it reshapes tensor so the first dimension remains the same and the other dimensions are flattened\n",
        "    x = x.view(x.size(0), -1)\n",
        "    # forward prograte to fully connected layer\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    return self.fc3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkN_LOT6L3Vs"
      },
      "source": [
        "**Constructor**: It ineritrts from the nn.Module class which is the base class for all neural network modules in pytorch. It intializes the network layers and sets up the network's architecture. The setting a seed which helps you control randomness, it intitial value for number generator which is 42. The network has four convolutional layers and four batch normalization layers. The first layer of convulational takes 3 input channels (rgb), 32 output channels. The kernel_size defines the size of the filter that captures features in the input such as edges and textures. The stride controls how much the kernel moves across the input. Higher stride controls produces smaller oupits while lower stides keep output closer in the size to the input. This steps up the 4 convulation layers and the input, output, kernel size, and stride. Output increases where as the kernal and stride decreases. The output increases as you go deeper into convolutional neural network, to capture more complec patterns gets more detailed features and increasing channels. The batch normalizion layer normalizes the outputs across each mini-batch, helping to stablize and speed up training. It will calculate the mean and standard deviation of all activations and use these values to scale the ouput to a mean of 0 and standard deviation of 1 for faster training. It reduces overfitting and improve generalization of the model - more stable and efficent. After the 4th batch normalization operation, fully connected layers process the flattened vector and output predicition to a vetcor of 512 - reduces the dimensionality of the feature vector while retaining key information. It futher reduces the feature vector size for the final prediction and then finally gets the action size that prodyces the final predicitions.\n",
        "\n",
        "Kernel is a small window that slides over an image. The kernel looks at the 3*3 patch, multiplies the pixel values by its own number (weights) and adds them up to get a new value which becomes a pixel in the ouput. Different kernels detect different features like edges or patterns. It is zoomed into the image and has a filter to analyze it and how close it is to the filter, slides over to each block - creates a process called pooling -each time the filter can be more specfic.\n",
        "\n",
        "Stride is how many pixels the kernel moves over each time. The stride of 1 means the kernel moves 1 pixel at a time, overlapping a lot with its previous position. 2 means 2 pizels at a time, covering mover at time and smaller output.\n",
        "\n",
        "\n",
        "\n",
        "**Forward**: This defines the foward pass for the neural netowrk in pytorch. It describes how the input data is processed through the neural network layer by layer to produce the final output. The input to the network typically an image or a batch of images. Each convolutional layers with baych normalization and activation. Each convolutional layer is followed by batch normalization and activation function. It applies the convulational layer to input x and normalizes the output to stablize training and improve learning. It applies relu activation function which introduces non-linearlity and helps with the network learn complex patterns. The negative values are replaced with zero instead.There is flattening tensor which converts the multi-dimensional output tensor from convolutional layers into 1 d vetcor for each image in the batch. The x.size(0) represents batch size unchanged while flattenign the rest of the dimensions. After flattneing it is passed through a series of fully connected layers to make predictions. It returns the nmber of actions the network is predicting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUvCfE_mhwo2"
      },
      "source": [
        "## Part 2 - Training the AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWCDPF22lkwc"
      },
      "source": [
        "### Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx8WQ7lbeY_z",
        "outputId": "c51be9f4-187f-46ff-8b11-244078632a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State shape:  (210, 160, 3)\n",
            "State size:  210\n",
            "Number of actions:  9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:517: DeprecationWarning: \u001b[33mWARN: The environment MsPacmanDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        }
      ],
      "source": [
        "import ale_py\n",
        "import gymnasium as gym\n",
        "# small part will be determinstic and full action space to be false and be more simply\n",
        "# easier to train the model\n",
        "env = gym.make('MsPacmanDeterministic-v0', full_action_space = False)\n",
        "state_shape = env.observation_space.shape\n",
        "state_size = env.observation_space.shape[0]\n",
        "number_actions = env.action_space.n\n",
        "print('State shape: ', state_shape)\n",
        "print('State size: ', state_size)\n",
        "print('Number of actions: ', number_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCXt3TLBU7l7"
      },
      "source": [
        "It is 210 * 160 for width of the pictures and rgb of 3. The state size is 210 and number of actions are 9."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx6IdX3ciDqH"
      },
      "source": [
        "### Initializing the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX3WJneTVKos"
      },
      "outputs": [],
      "source": [
        "# Chose this after much experimenatation to get this learning rate\n",
        "learning_rate  = 5e-4\n",
        "# Number of the observation in one step to update the model's parameter\n",
        "# Common pratice is 64 usual size - no optimal size\n",
        "minibatch_size = 64\n",
        "# Present value of future rewards\n",
        "# Small makes it shortsighted and only look at current rewards\n",
        "# Closer to 1 will make it look at future rewards in accumulation to total reward\n",
        "# Want to do this instead of short sighted\n",
        "discount_factor = 0.99\n",
        "# wont do soft update this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-ekqT7XaJmx"
      },
      "source": [
        "Same parameters as before almost, the minibatch size will decrease to around 64. The experience replay buffer size will not be needed and will be done in a simple way. Interpolation parameter does not help for soft update as this will not improve for the learning model for pacman"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2bDShIEkA5V"
      },
      "source": [
        "### Preprocessing the frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQiHoKS8ZBPt"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "    # Convert the frame to a PIL Image object if it's not already\n",
        "    # now will be a numpy array to PIL image object\n",
        "    frame = Image.fromarray(frame)\n",
        "    # Will be proprocessing which contains a list of transformation that will be doing\n",
        "    # It will be resize of our frames  which will be 210 * 160 - need make squares and smaller\n",
        "    # convert into a pytorch tensor and normalize the frames 0-1\n",
        "    preprocess = transforms.Compose([transforms.Resize((128, 128)), transforms.ToTensor()])\n",
        "    return preprocess(frame).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU4ZNqEsc2bS"
      },
      "source": [
        "This does proprocessing steps to make it more suitable for pytorch model. The input is a frame is expected to be a numpy array. It does converstion which converts numpy array into PIL image object. The step is necessary because pytorch's transformation. It combines multiple precressing steps in a single pipeline. These transformation are applied sequentially into input frame. It resizes image to a fixed size of 128 x 128 pixels. It passed through the network have a uniform size which is requirement for most neural networks as the input images might originally have different dimensions. It finally converts the PIL image into a pytorch tensor. It adds a batch dimension at the"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VavV_QZKYZF6"
      },
      "source": [
        "### Implementing the DCQN class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbRVi815duk6"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "  def __init__(self, action_size):\n",
        "    # using gpu if available else we are using a cpu`\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.action_size = action_size\n",
        "    # instance of netural network class to create the local and target q network and send to our device\n",
        "    self.local_qnetwork = NeuralNetwork(action_size).to(self.device)\n",
        "    self.target_qnetwork = NeuralNetwork(action_size).to(self.device)\n",
        "\n",
        "    # instance of the Adam to create an optimizer and update the weights to predict the weights to land on move\n",
        "    # learning rate is inputted too\n",
        "    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate)\n",
        "    self.memory = deque(maxlen = 10000)\n",
        "\n",
        "\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    # preprocess the states to add them to the memory do it for both state and next state\n",
        "    state = preprocess_frame(state)\n",
        "    next_state = preprocess_frame(next_state)\n",
        "    # append into memory\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "    if len(self.memory) > minibatch_size:\n",
        "      experiences = random.sample(self.memory, k = minibatch_size)\n",
        "      self.learn(experiences, discount_factor)\n",
        "\n",
        "\n",
        "\n",
        "    # select action depending on a given state and action selection policy\n",
        "  def act(self, state, epsilon = 0.):\n",
        "    state = preprocess_frame(state).to(self.device)\n",
        "    # instance of netural network class which is from the nn.module class\n",
        "    # set it into eval mode and foward pass the state [] -> [[]]\n",
        "    self.local_qnetwork.eval()\n",
        "    # check it is inference mode for making predictions not the training mode\n",
        "    with torch.no_grad():\n",
        "      action_values = self.local_qnetwork(state)\n",
        "    self.local_qnetwork.train()\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "    # foward pass those values into the local network and give the actions from that network - gives final action\n",
        "    # set to local training mode\n",
        "    # Using epilson greedy this is done by if the random number is larger than epilson pick largest q value\n",
        "    # otherwise pick a random action\n",
        "\n",
        "  def learn(self, experiences, discount_factor):\n",
        "    # will unziip experiences which is in the memory deque as the memeory\n",
        "    # states and next_states are already pytorch sensors\n",
        "    states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "\n",
        "    # you will want to get a stack of the states, actions, rewards, next states, and dones\n",
        "    states = torch.from_numpy(np.vstack(states)).float().to(self.device)\n",
        "    actions = torch.from_numpy(np.vstack(actions)).long().to(self.device)\n",
        "    rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device)\n",
        "    next_states = torch.from_numpy(np.vstack(next_states)).float().to(self.device)\n",
        "    dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(self.device)\n",
        "\n",
        "\n",
        "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    # action values of next states and detach function actual detached tensor from computation graph in order to get the maximum\n",
        "    # get max of each row and it returns the max and the indices so get the actual max\n",
        "    # adds a dimension after each feature, [] , [[],[],[]]\n",
        "\n",
        "    # compute the q target for our current state\n",
        "    q_targets = rewards + discount_factor * next_q_targets * (1 - dones)\n",
        "    q_expected = self.local_qnetwork(states).gather(1, actions)\n",
        "    # calculate the loss\n",
        "    loss = F.mse_loss(q_expected, q_targets)\n",
        "    # reset it\n",
        "    self.optimizer.zero_grad()\n",
        "    # backprograte the loss\n",
        "    loss.backward()\n",
        "    # does one single optimization step\n",
        "    self.optimizer.step()\n",
        "    # allow it update the target network slowly and follow improvements withput changing too rapidly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2SWPocTY0Lw"
      },
      "source": [
        "The DCQN agent implementation for reinforcement learning where an agent learns to take actions in an environment to maximize cumulative rewards using a neural netowrk as a function approximator.\n",
        "\n",
        "**Constructor:**\n",
        "- Device setup to uses gpu otherwise defaults to cpu\n",
        "- Q networks\n",
        "  - Local q networks - used for policu evaluation during trainign\n",
        "  - Target q-network - used for stable q valye updates to mitigate the instability\n",
        "  - Both networks are instances of Neural Netowrk sent to self.device\n",
        "- Optimizer: used for updating the weights of the local qnetwork\n",
        "- Relay Memory: deque object stores transitions with fixed maximum size to implement experience\n",
        "\n",
        "\n",
        "**Step**\n",
        "- Stores transitions in memory\n",
        "- samples a minibatch experiences once the memory size exceeds the defined minibatch sized\n",
        "- calls the learn method to train the method used the sampled experiences\n",
        "\n",
        "**Act**\n",
        "- Uses a epsilon greedy strategy\n",
        "- With prob 1-e, select the actions with the highest q value predicted by local network\n",
        "- otherwise selects a random action\n",
        "- ensures that exploration occurs during trainign while exploiting the learned policu\n",
        "\n",
        "**Learn**\n",
        "- Processes a batch of experiences\n",
        "- target q values: calculate using the target netowrk\n",
        "- Expected Q-values: Predicted by the local network (local_qnetwork) based on the actions taken.\n",
        "- Loss Function: Mean Squared Error (MSE) between the target Q-values and expected Q-values.\n",
        "- Updates the local_qnetwork using backpropagation and a gradient descent step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUg95iBpAwII"
      },
      "source": [
        "### Initializing the DCQN agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVUupa1UfJN7"
      },
      "outputs": [],
      "source": [
        "agent = Agent(number_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK6Zt_gNmHvm"
      },
      "source": [
        "### Training the DCQN agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG79350tfhuh",
        "outputId": "fdc43bc0-7a39-4b4d-b682-15833c4f950e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 100\tAverage Score: 290.30\n",
            "Episode 200\tAverage Score: 363.90\n",
            "Episode 300\tAverage Score: 371.50\n",
            "Episode 400\tAverage Score: 458.00\n",
            "Episode 500\tAverage Score: 434.60\n",
            "Episode 600\tAverage Score: 424.30\n",
            "Episode 700\tAverage Score: 422.80\n",
            "Episode 800\tAverage Score: 378.30\n",
            "Episode 900\tAverage Score: 435.10\n",
            "Episode 1000\tAverage Score: 395.80\n",
            "Episode 1100\tAverage Score: 452.00\n",
            "Episode 1200\tAverage Score: 404.00\n",
            "Episode 1219\tAverage Score: 427.40"
          ]
        }
      ],
      "source": [
        "# max number of episodes\n",
        "number_episodes = 2000\n",
        "# max time per episodes\n",
        "maximum_number_timesteps_per_episode = 10000\n",
        "epsilon_starting_value = 1.0\n",
        "epsilon_ending_value = 0.01\n",
        "epsilon_decay_value = 0.995\n",
        "epsilon = epsilon_starting_value\n",
        "scores_on_100_episodes = deque(maxlen = 100)\n",
        "# decrement epsilon\n",
        "\n",
        "for episode in range(1, number_episodes + 1):\n",
        "  # reset to initial state\n",
        "  state, _ = env.reset()\n",
        "  score = 0\n",
        "  for t in range(maximum_number_timesteps_per_episode):\n",
        "    action = agent.act(state, epsilon)\n",
        "    # act selcts an action in a given state following an epsilon greedy policy\n",
        "    # the gynasium calculates the reward, next state, done, all from the environment step and takes this and uses it for the agent step\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    # shows that action does to the environmet\n",
        "    # agent acts on this and learns on the current state to next state\n",
        "    agent.step(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if done:\n",
        "      break\n",
        "  # append of the score of the action taken\n",
        "  scores_on_100_episodes.append(score)\n",
        "  # decrements the epsilon\n",
        "  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
        "  # dynamic print - removed to make room and \\r to loop back\n",
        "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = \"\")\n",
        "  if episode % 100 == 0:\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
        "  if np.mean(scores_on_100_episodes) >= 500.0:\n",
        "    print('\\nEnivornment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n",
        "    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
        "    break\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0WhhBV8nQdf"
      },
      "source": [
        "## Part 3 - Visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb9nVvU2Okhk"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'MsPacmanDeterministic-v0')\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}